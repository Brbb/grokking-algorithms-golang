# k-nearest neighbors

## Classification
- You can use the KNN algorithm to classify stuff, like if a fruit is an orange or a grapefruit
- In this example, oranges are orange and smaller than grapefruits, which are red
- So in a graph, you'd have two axis: Y for color and X for size. Oranges would be in the bottom-left and grapefruits in the up-right
- What if you have a fruit right in the middle? How to classify? You can look at its _neighbors_ and count the closest ones, if there are more oranges, it's an orange, otherwise, it's an apple

## Recommendation
- You can also use KNN to recommend stuff, like recommending a movie on Netflix
- To do that, we can put _K_ Netflix users in a graph and recommend movies to a specific user based on its neighbors
- How to plot the user on a graph? We need to extract _features_ from the user:
    - Features could be something like color and size in the fruit example
        - In this example, the distance has _two_ dimensions, so we can calculate the distance using the `sqrt((x1 - x2)^2 + (y1-y2)^2)`
    - Or it could be rating for movies in this example
        - In this example, the features can be the rating for movie categories, which could have more dimensions, but the formula continues the same
- So now you have the distance between each user, just recommend movies from neighbors to each other

- You can also add weight to one user, if she is an influencer
- Also, more users better data!

## Regression
- Like predicting the future
- You can get an average from a set of features from the neighbors and try to predict the value for the user

---

- Picking good features is very important! In the Netflix example, if you only pick values based on comedy movies, you probably wouldn't know how to recommend action movies
- This is an intro do Machine Learning! Techniques like this are used for OCR, spam filters, predicting market stock etc

## Resources
[KNN Algorithm using Python | Edureka](https://www.youtube.com/watch?v=6kZ-OPLNcgE)